export const meta = {
	description: "Notes on algorithms for arrays.",
	title: "Sequences",
};

<meta name="description" content="Notes on algorithms for arrays." />
<title>Sequences</title>

## Sequence ADT: Auxiliary Functions

Implementing our sequence ADT as a class, there are a few auxiliary
functions that we will use as helpers for more complex procedures. These
helper functions are explained below.


## Shifting & Rotating

The next operations we consider are `leftShift()`, `rightShift()`,
`leftRotate()`, and `rightRotate()`. Each of these operations involves
moving elements to the left or right—particularly useful actions when
implementing other algorithms (e.g., filling in holes in the array).









#### Negatives Towards Start & Positives Towards End

One problem that often appears with array data structures is where we have
a mixture of positive and negative numbers. Confronting this problem, we
often want to move all the negative numbers towards the beginning, or the
right side, and the positive numbers towards the end. Note that we aren't
sorting the elements. We're simply separating the negatives from the
positives. For example, such an array might look like the following:

<Sequence data={[-6, 3, -8, 10, 5, -7, -9, 12, -4, 2]} />

What we want to do is obtain an array that looks like:

<Sequence data={[-6, -4, -8, -9, -7, 5, 10, 12, 3, 2]} />

Notice that we aren't sorting. We're just moving the positive and negative
elements. We will call this operation `signSplit()`. With this operation,
we will iterate. First, we create two variables: A variable `i` to track
the indices starting from the left, and a variable `j` to the track the
indices from the right. Then, we iterate through the array, as long as
`i < j.`. Then, inside the while-loop, we place two more while-loops. As
long as `A[i] < 0,` we will increment `i`. Otherwise, we go the second
while-loop: As long as as `A[j] > 0,`, we will decrement `j`. Then, once
that has finished executing, we will swap `A[i]` with `A[j]`. This process
continues as long as `i < j.` The implementation:

```rust
signSplit():
	int i = 0;
	int j = length - 1;
	while (i < j):
		while (array[i] < 0): i++;
		while (array[j] <= 0): j--;
		swap(array[i], array[j]);
```

The implementation above is a good example of how nested loops do not
always imply polynomial time. Here, the time spent most is just on
comparing the elements. In this case, there are a total of ${n + 2}$
comparisons (+2 because both `i` and `j` must both check one extra number).
Because the brunt of this time is spent on comparisons of ${n}$ elements,
this approach has a time complexity of $ {O(n)}$—linear time. To see that
this is the case, the code above is equivalent to the following:

```rust
signSplit():
	int i = 0;
	int j = length - 1;
	while (i ${<}$ j):
		if (array[i] < 0) & (array[j] < 0):
			i++;
		else if (array[i] > 0) & (array[j] > 0):
			j--;
		else if (array[i] > 0) & (array[j] < 0):
			swap(array[i], array[j]);
		else:
			i++;
			j--;
```

With this implementation, we can clearly see that the algorithm has a time
complexity of ${O(n).}$ We used the multiple while-loop approach simply
because it's cleaner. Alternatively, here's another implementation that
takes an approach similar to the partitioning algorithm of quicksort (an
algorithm we will see in due course):

```rust
signSplit():
	int j = 0;
	for (int i = 0; i < length; i++):
		if (arr[i] < 0):
			if (i ${\neq}$ j):
				swap(arr[i], arr[j]);
			j++;
```

Clearly with this implementation, we still have a time complexity of
${O(n).}$

## Binary Operations on Arrays

In this section, we consider the _binary operations_ on arrays. These are
operations that involve two arrays as operands, namely: `merge(A, B)`;
`append(A, B)`; `concat(A, B)`; and `copy(A)`, where `A` and `B` are
arrays. We consider each in turn.

## Appending an Array.

With the `A.append(B)` operation, we take an existing array `A` and add on
to it an array `B.` For example, suppose we had some array `A`:

<Sequence data={[2, 3, 9, 1, "", "", "", ""]} />

Then suppose we had the array `B`:

<Sequence data={[4, 6, 7]} />

With the `A.append(B)` operation, we receive as output the following array:

<Sequence data={[2, 3, 9, 1, 4, 6, 7]} />

Because we're working with a static array data structure, this operation
works _only if_ the array to append to (in this case `A`), has
uninitialized positions. In other words, the array's _length_ is less than
its _size_. Otherwise, we would go beyond the array's bounds. Implementing
this algorithm:

```rust
append(A[], B[]):
	if (A.length < A.size):
		int j = 0;
		for (int i = A.length+1; i < size; i++):
			A[i] = B[j];
			A.length++;
			j++;
	else:
		return "A has no free spaces";
```

Here, we must perform ${n = s - \ell}$ comparisons for the index `i`, where
${s}$ is the size of the array and ${\ell}$ is the length of the array.
Accordingly, this algorithm has a time complexity of ${O(n),}$ linear time,
where ${n}$ is the difference between the array's size and its length.

## Merging Arrays.

Say we had the following arrays:

<Sequence data={[3, 8, 16, 20, 25]} />
<Sequence data={[4, 10, 12, 22, 23]} />

We want to take these two arrays and construct the following array:

<Sequence data={[3, 4, 8, 10, 12, 16, 20, 22, 25, 23]} />

This operation is called **array merging**, and we denote it with the
operator `merge(A, B)`, where `B` is the array to be merged on to the array
`A`. Notice the outpout of `merge()`: It's an array where the elements of
`A` and `B` are placed into a single array, sorted. Accordingly, this
operation requires a third array for its return.

How does this operation work? Here, we need three pointers: `i`, `j`, and
`k` for each of the arrays `A`, `B`, and the new array, `C`. The pointer
`i` will track the indices of `A`, the pointer `j` the indices of `B`, and
the pointer `k` the indices of `C`. Once these are established, we iterate,
starting from the first position. If `A[i] < B[j]`, then: `A[i]` is
assigned to `C[k]`, and both `i` and `k` are incremented. Otherwise, we
assign `B[j]` to `C[k]`, and both `j` and `k` are incremented. Notice that
with this procedure, `k` is always incremented. Whether `i` or `j` is
incremented depends on whether the element at that particular position is
less than the other. The implementation:

```rust
merge(A, B):
	new array C = A.length + B.length
	int i = 0, j = 0, k = 0;
	while (i < A.length & j < B.length):
		if (A[i] < B[j]):
			C[k] = A[i];
			i++;
		else:
			C[k] = B[j];
			j++;
		k++;
	for (i < A.length; i++):
		C[k++] = A[i];
	for (j < B.length; j++):
		C[k++] = B[j];
```

Notice that we included two more for-loops at the end. This is to handle
the "stragglers"—the elements remaining in either array `A` or array `B`
that could not be copied over because the while-loop terminated before both
arrays were completely compared. This occurs if either `A` or `B` has three
elements in a row that are less than the given element in the other. For
example, if `A = [1, 2]` and `B = [3, 4]`, then the while-loop would
terminate the moment `i = 1` (at which point `k = 1`), and as such, the
elements of `B` are not checked. The for-loop ensures the unchecked
elements are checked and assigned accordingly.

The time complexity for this operation is straightforward. We're comparing
${m}$ elements and ${n}$ elements (the two array arguments). Accordingly,
this algorithm has a time complexity of ${\Theta(m + n);}$ linear time.
Whenever we see a time complexity of the form ${a + b,}$ it's highly likely
that we're dealing with an algorithm that involves merging.

## Set Operations on Arrays

With arrays, we can also perform the fundamental set operations of _union_,
_intersetion_, _difference_, and _membership_. We will call these
operations as such: `union(A, B)`, `intersection(A, B)`,
`difference(A, B)`, and `isMember(x, A)`.

## Union of Arrays.

Recall that in mathematics, given a set ${A = \{ 3, 5, 10, 4, 6 \}}$ and a
set ${B = \{ 12, 4, 7, 2, 5 \},}$ the union of ${A}$ and ${B}$ is the set
${A \cup B = \{ 3, 5, 10, 4, 6, 12, 7, 2 \}.}$ Notice that the union lists
no duplicates. In this case, there were two duplicates: ${4}$ and ${5.}$
These duplicate elements were omitted from the resulting set.

We implement the process of forming a union through the operator
`union(A, B)`, where `A` and `B` are arrays. Implementing this operator, we
want to keep in mind the requirement that there must be no duplicates in
the resulting array. We aren't just blindly copying. Otherwise, the
operator would be no different from `concatenate()`.

Suppose the array `A` is the following:

<Sequence data={[3, 5, 10, 4, 6]} />

And the array `B`:

<Sequence data={[12, 4, 7, 2, 5]} />

Executing `union(A, B)`, we have:

<Sequence data={[3, 5, 10, 4, 6, 12, 7, 2]} />

The algorithm consists of several steps. First, we have to assign all of
the elements in the first array to the new array. If there are ${m}$
elements, this process takes ${m}$ time. Then, to ensure we do not have any
duplicates, we must check the ${n}$ elements in the second array against
the ${m}$ elements inside the new array then assign the elements that do
not match. Given ${m}$ elements in the new array, this checking takes
${m \cdot n}$ time. Accordingly, the algorithm has a runtime complexity of
${O(m + nm).}$ To simplify this expression for the purposes of asymptotic
analysis, we want to only use one variable. Accordingly, the time
complexity is ${O(n + nn),}$ or more relevantly, ${O(n^2).}$ This is
quadratic time.

To improve this runtime, we can sort the array elements before performing
the union. We will cover sorting algorithms in a later section, so for now,
we will just assume that the array elements are already sorted. Suppose the
array ${A}$ is:

<Sequence data={[3, 4, 5, 6, 10]} />

And the array ${B:}$

<Sequence data={[2, 4, 5, 7, 12]} />

With sorted arrays, all we have to do is merge while avoiding duplicates.
We compare ${A_i}$ against ${B_j.}$ The lesser element is assigned, and the
other element's index is incremented. If both elements are equal, we assign
either one, and increment both indices. Thus:

1. Start:

   - ${i = 0,}$ the index for ${A,}$
   - ${j = 0,}$ the index for ${B,}$ and
   - ${k = 0,}$ the index for ${C.}$

2. Compare ${A_i \leq B_j.}$

<Sequence data={[3, 4, 5, 6, 10]} />
<Sequence data={[2, 4, 5, 7, 12]} />

3. ${3}$ is not less than or equal to ${2,}$ so assign ${2}$ to the array
   ${C,}$ which is ${B_j.}$ Increment ${j}$ and ${k.}$ ${j}$ is now ${1}$
   and ${k}$ is now ${1.}$

<Sequence data={[3, 4, 5, 6, 10]} />
<Sequence data={[2, 4, 5, 7, 12]} />

<Sequence data={[2]} />

4. Compare ${A_i \leq B_j.}$

<Sequence data={[3,4,5,6,10]}/>
<Sequence data={[2,4,5,7,12]}/>
	
5. ${3}$ is less than ${4,}$ so assign ${3}$ to the array ${C,}$ which is ${A_j.}$ Increment ${i}$ and ${k.}$ ${i}$ is now ${1}$ and ${k}$ is now ${2.}$

<Sequence data={[3, 4, 5, 6, 10]} />
<Sequence data={[2, 4, 5, 7, 12]} />
<Sequence data={[2, 3]} />

6. Compare ${A_i \leq B_j.}$

<Sequence data={[3, 4, 5, 6, 10]} />
<Sequence data={[2, 4, 5, 7, 12]} />

7. ${4}$ is equal to ${4,}$ so assign ${4}$ to the array ${C,}$ which is
   ${A_j.}$ Increment ${i,}$ ${j,}$ and ${k.}$ ${i}$ is now ${2,}$ ${j}$ is
   now ${2,}$ and ${k}$ is now ${3.}$

<Sequence data={[3, 4, 5, 6, 10]} />
<Sequence data={[2, 4, 5, 7, 12]} />
<Sequence data={[2, 3, 4]} />

8. Compare ${A_i \leq B_j.}$

<Sequence data={[3, 4, 5, 6, 10]} />
<Sequence data={[2, 4, 5, 7, 12]} />

7. ${5}$ is equal to ${5,}$ so assign ${5}$ to the array ${C,}$ which is
   ${A_j.}$ Increment ${i,}$ ${j,}$ and ${k.}$ ${i}$ is now ${3,}$ ${j}$ is
   now ${3,}$ and ${k}$ is now ${4.}$

<Sequence data={[3, 4, 5, 6, 10]} />
<Sequence data={[2, 4, 5, 7, 12]} />
<Sequence data={[2, 3, 4, 5]} />

8. Compare ${A_i \leq B_j.}$

<Sequence data={[3, 4, 5, 6, 10]} />
<Sequence data={[2, 4, 5, 7, 12]} />

9. ${6}$ is less than ${7,}$ so assign ${6}$ to the array ${C,}$ which is
   ${A_j.}$ Increment ${i}$ and ${k.}$ ${i}$ is now ${4,}$ ${j}$ is still
   ${3,}$ and ${k}$ is now ${5.}$

<Sequence data={[3, 4, 5, 6, 10]} />
<Sequence data={[2, 4, 5, 7, 12]} />
<Sequence data={[2, 3, 4, 5, 6]} />

The procedure continues for the rest of the elements. Because this approach
to `union()` is essentially merging two arrays, it has a runtime of
${O(m+n),}$ or more generally ${O(n).}$ This is linear time, which is
better than quadratic time.

## Intersection.

From set theory, the intersection of two sets ${A}$ and ${B}$ is the set of
all elements in both ${A}$ and ${B.}$ For example, suppose
${A = \{ 1, 2, 3 \}}$ and ${B = \{ 1, 2, 4 \}.}$ The intersection
${A \cap B}$ is the set ${\{ 1, 2 \}.}$ Like the `union()` operation, we
can implement the intersection operation with arrays. We'll call this
operation `intersection(A, B)`, where `A` and `B` are arrays.

To illustrate, suppose ${A}$ and ${B}$ were the following. The array ${A:}$

<Sequence data={[3, 5, 10, 4, 6]} />

And the array ${B:}$

<Sequence data={[12, 4, 7, 2, 5]} />

We then have an empty array ${C:}$ which will store the intersection. An
important question is what should be the size of ${C?}$ The size of ${C}$
is computed from the _Cardinal Number Formula:_

$$
	n(A \cap B) = n(A) + n(B) - n(A \cup B)
$$

Applying the formula, the size of ${C}$ is the length of ${A}$ plus the
length of ${B,}$ minus the length of the array resulting from the union of
${A}$ and ${B.}$ While this would give us the exact number of spaces
necessary for the intersection, it would be simpler, and more efficient, to
use the length of whichever is smaller (in terms of the number of elements)
between ${A}$ and ${B.}$ This is because the number of elements in common
between ${A}$ and ${B}$ is at most the cardinality of the smaller between
the two sets. If ${A}$ is a smaller set than ${B,}$ then the cardinality of
${A \cap B}$ is at most the cardinality of ${A}$ (all the elements in ${A}$
are elements in ${B}$). If ${B}$ is a smaller set than ${A,}$ then the
cardinality of ${A \cap B}$ is at most the cardinality of ${B.}$ And if
${A}$ and ${B}$ are of the same cardinality, then the cardinality of
${A \cap B}$ is at most the cardinality of ${A}$ (or ${B}$).

In our case, the arrays ${A}$ and ${B}$ are of the same length, so the
length of ${C}$ is at most ${5}$ (the elements of ${A}$ are all elements
${B}$ and vice versa):

<Sequence data={["", "", "", ""]} />

We start by To output the intersection, we compare each element in ${A}$
against each element in ${B.}$ If there's a match, then: (1) we assign the
element in ${A}$ to the position in ${C,}$ and (2) increment ${k,}$ the
index for ${C.}$ Comparing element in ${A}$ to each element in ${B}$
requires ${n \cdot m}$ operations. As such, this algorithm has a time
complexity of ${O(n^2).}$ This isn't very efficient; it's quadratic time.

Like the `union()` operation, we can improve this runtime if the arrays are
sorted before performing `intersection()`. Suppose our previous arrays were
sorted:

<Sequence data={[3, 4, 5, 6, 10]} />

<Sequence data={[2, 3, 5, 7, 12]} />

`intersection()` then works as such:

1. Set `i = 0`, `j = 0`, and `k = 0`.
2. Compare: `A[i]` and `B[j]`.
   1. If `A[i] > B[j]` increment `j`.
   2. If `A[i] < B[j]` increment `i`.
   3. If `A[i] = B[j]`, assign `A[i]` to `C[k]`, increment `i` `j`, and
      `k`.
   4. If `i > A.length` or `j > B.length`, return `C.` Else, return to
      step 2.

With this approach, we have time complexity of ${O(m + n),}$ or more
generally, ${O(n).}$ This is linear time; a marked improvement from
quadratic time.

## Difference.

Suppose we had the following sets:

$$
\begin{aligned}
	A &= \{ 3, 5, 10, 4, 6 \} \\
	B &= \{ 12, 4, 7, 2, 5 \}
\end{aligned}
$$

The difference between these two sets is the set consisting of elements not
in both ${A}$ and ${B}$ (i.e., the set without the elements common to both
${A}$ and ${B}$). In this case, the common elements are ${4}$ and ${5.}$
Thus, ${A - B = \{ 3, 10, 6 \},}$ and ${B - A = \{ 12, 7, 2 \}.}$ We
implement this operation as the operator `difference()`.

With the `difference()` operator, we say that order is material. Thus,
`difference(A, B)` computes ${A - B,}$ and `difference(B, A)` computes
${B - A.}$ In other words, the arguments to the `difference()` operator are
noncommutative.

Like `intersection()`, we might implement `difference()` by comparing each
element in ${A}$ against each element in ${B.}$ If an element in ${A}$
matches an element in ${B,}$ we skip that element compare the next. If
there is no match, we assign the element to the resulting array ${C.}$
Again, like `intersection()`, this algorithm requires a total of
${n \cdot m}$ operations to execute. This yields a quadratic runtime of
${O(n^2).}$

Just as we saw with `intersection()`, we can improve the time complexity by
ensuring the array arguments are sorted before performing the
`difference()` procedure. Suppose the arrays are sorted:

<Sequence data={[3, 4, 5, 6, 10]} />
<Sequence data={[2, 4, 5, 7, 12]} />
<Sequence data={[0, 1, 2, 3, 4]} />

With the arrays sorted, the `difference()` operator's algorithm works as
such:

1. Set `i = 0`, `j = 0`, and `k = 0`.
2. Compare `A[i]` with `B[j]`.
   1. If `A[i] > B[j]`, increment `j`.
   2. If `A[i] < B[j]`:
      1. Assign `A[i]` to `C[k]`.
      2. Increment `i`.
      3. Increment `k`.
   3. If `A[i] = B[j]`, increment `i`, increment `j`.
   4. If `i > A.length`, return `C`. Else, return to Step 2.

The algorithm above computes the difference ${A - B.}$ As such, when
difference is called, it is called as `difference(A, B)`. To compute
${B - A,}$ we perform the same algorithm, but swap `A[i]` and `B[j]` (and
their respective indices) in the algorithm above.

This algorithm performs in the same manner as the `merge()` operator.
Accordingly, it has a time complexity of ${O(m + n),}$ or, in a single
variable, ${O(n).}$ This is linear time; faster than quadratic time.

## Search-and-rescue Algorithms: Algorithms for Finding Missing Elements

A particularly useful set of algorithms associated with arrays is the set
of algorithms for finding missing elements. As an homage to
search-and-rescue dogs, we call these algorithms **search-and-rescue
algorithms**.

Search-and-rescue algorithms are particularly useful when we know that a
particular sequence should contain certain elements. For example, a
password, identification number, or waiting list number might have a
pre-defined rule for what the sequence of digits should be. If the sequence
is sufficiently long, it would be tedious for us to check, by hand, each
element one by one just to spot a missing element.

This is where the search-and-rescue algorithm comes in. Implementing these
algorithms depends on four factors: (1) the sequence (implemented as an
array) to check is sorted; (2) the sequence is unsorted; (3) the sequence
contains a single missing element; and (4) the sequence contains multiple
missing elements. For this section, we restrict the search-and-rescue
algorithms to operating on sequences of numbers. However, our discussion
should shed light on how these algorithms might be extended to arrays of
other values, such as strings.

#### Sorted Search-and-rescue: Single Missing Elements

First, we consider a sequence implemented as a sorted array. Suppose the
sorted array is the following:

<Sequence data={[1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12]} />

This is the sequence of natural numbers. Although we immediately see that
the missing element is ${7,}$ the task would be far more difficult if ${A}$
consisted of a thousand elements. The trick to finding the missing element
in sorted array is to rely on the sequence's summation formula. In this
case, the sum of the natural numbers is given by the following:

$$
	\sum\limits_{k = 1}^{n} k = \dfrac{n(n+1)}{2} \qquad \text{where} ~~~~ k, n \in \N
$$

Knowing that ${A}$ is the sequence of the natural numbers, we can find the
sum of ${A}$ if the missing element was present. We'll call this the
_expected sum_. In this case, the expected sum of the array ${A}$ would be:

$$
	\sum\limits_{i = 1}^{12} A_i = \dfrac{12(12 + 1)}{2} = 78
$$

Next, we compute the sum of ${A}$ as is (without the missing element
included). We'll call this the _current sum_. To do so, we use our familiar
`sum()` operator. In this case, the _current sum_ is ${71.}$ Once we obtain
the current sum, the missing element is the difference between the
_expected sum_ and the _current sum_:

$$
	\large A_{\text{missing}} = S_{\text{expected}} - S_{\text{current}}
$$

Where:

- ${A_{\text{missing}}}$ is the missing element;
- ${S_{\text{expected}}}$ is the expected sum; and
- ${S_{\text{current}}}$ is the current sum.

Applying this formula:

$$
	\begin{aligned} \large A_{\text{missing}} &= S_{\text{expected}} - S_{\text{current}} \\ &= 78 - 71 \\ &= 7 \end{aligned}
$$

The implementation might look like:

```rust
sum_natural_numbers(n):
	return ((n * (n + 1)) / 2);
	findMissing(A, summation_formula()):
		let current_sum = 0;
		for (i = 0; i < A.length - 1; i++):
			current_sum += A[i];
	let expected_sum = summation_formula(A.lastElement)
	let missing_element = expected_sum - current_sum;
	return missing_element;
	findingMissing(A, sum_natural_numbers());
```

With the implementation above, we rely on a separate function,
`sum_natural_numbers(n)`, to compute the expected sum. There may be
situations, however, where we do not know the summation formula for a
particular sequence. Moreover, the call to `sum_natural_numbers(n)`
requires an additional stack frame, and we may have space complexity
concerns (certain summation formulas can get very complex or require large
amounts of memory). As such, we consider an alternative implementation.

With the alternative implementation, we rely on the indices of each element
in the array. For example, suppose we had the following array:

<Sequence data={[6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]} />

Notice with each element in the array, the difference between the element
and its index is 6, until we get to the element at index ${6.}$

$$
	\begin{aligned} 6 - 0 &= 6 \\ 7 - 1 &= 6 \\ 8 - 2 &= 6 \\ 9 - 3 &= 6 \\ 10 - 4 &= 6 \\ 11 - 5 &= 6 \\ 13 - 6 &= 7 \end{aligned}
$$

The break at ${13 - 6}$ indicates there's a missing element. The missing
element is given by ${6 + 6,}$ which is ${12.}$ Implementing this approach:

```rust
findMissing(arr A):
	let low = A.firstElement;
	let difference = low - 0;
	for (let i = 0; i < A.length; i++):
		if (A[i] - i != difference):
			print "Missing element is: (i + difference)";
			break
		else: print "No missing elements";
```

With both the summation formula approach and the index difference approach,
we must iterate through ${n}$ elements. Accordingly, both algorithms have a
time complexity of ${O(n)}$—linear time.

#### Sorted Search-and-rescue: Multiple Missing Elements

Now let's consider finding multiple missing elements. Consider the
following array:

<Sequence data={[6, 7, 8, 9, 11, 12, 15, 16, 17, 18, 19]} />

We have several missing elements: ${10, 13, 14.}$ Not only do we have
multiple missing elements, we also have consecutive missing elements. To
sniff out the missing, we'll use the same index-difference approach. But,
we must account for a change in the problem's dynamics. The first occurence
of the missing element's scent is at ${i = 4.}$ This tells us that there's
a missing element: ${i + d = 4 - 6 = 10.}$

Equally important is the fact that this change in difference, from ${6}$ to
${7,}$ permeates through the rest of the sequence. Accordingly, we must
increment the difference: ${6 + 1 = 7.}$ The difference is now ${7.}$ At
${i = 5,}$ the difference is ${12 - 5 = 7,}$ so we can conclude that there
is no missing element. Accordingly, if there are no other missing elements,
the difference should remain ${7}$ on and after ${i = 4.}$ Clearly,
however, this isn't the case. At ${i = 6,}$ we have another change in the
difference: ${15 - 6 = 9.}$ Because another change occurs (the difference
should be ${7}$), we conclude that another element is missing:
${i + d = 6 + 7 = 13.}$ Once more we increment the difference:
${7 + 1 = 8.}$ At ${i = 6,}$ we see that ${15 - 6 = 9,}$ which is not
${8.}$ Hence, we have another missing element: ${i + d = 6 + 8 = 14.}$ Once
more we increment the difference—${8 + 1 = 9.}$ At ${i = 6,}$ we have
${15 - 6 = 9.}$ Because this is the correct difference, we can conclude
that the element is in the right position. This process yields the missing
elements: ${10, 13, 14.}$ Implementing:

```rust
findMissing(array A):
	let low = A.firstElement;
	let difference = low - 0;
	for (int i = 0; i < A.length; i++):
		if (A[i] - i != difference):
			while (difference < A[i] - i):
				print "Missing element is: (i + difference)";
				difference++;
		else: print "No missing elements";
```

Although the algorithm above contains a while-loop inside a for-loop, the
time taken to print the missing elements is negligible. The majority of the
time taken to print the missing elements lies in the for-loop, where we
must iterate through ${n}$ elements. Accordingly, this algorithm takes
linear time to execute, ${O(n).}$

### Bitsets: Search-and-rescue for Unsorted Arrays

Let's now consider how to find the missing elements of an unsorted array.
Suppose we had the following array:

<Sequence data={[3, 7, 4, 9, 12, 6, 1, 11, 2, 10]} />

To begin, we start by finding the largest element in the array. We'll call
this the `high`. In this case, it's ${12.}$ Then, we create a new array of
size ${13,}$ where each element in the array is ${0:}$

<Sequence data={[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]} />

Now we scan through the array ${A.}$ For each of the elements, we will
increment the ${0}$ occupying the corresponding index with the same value
in the array ${H.}$ For example, the first element in ${A}$ is ${3,}$ so we
go to the ${0}$ at ${j = 3,}$ and increment it by ${1.}$ This results in
the following array:

<Sequence data={[0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1]} />

The remaining zero elements in the array ${H}$ indicate holes in the
sequence. The array ${H}$ itself is more generally called a **bitset**. In
this case, we see that the missing elements are ${0,}$ ${5,}$ and ${8.}$ As
we can see, this algorithm runs on linear time. All we must do is iterate
through each of the elements in ${A,}$ then increment the corresponding
element in ${H.}$ Given ${n}$ elements, this has a time complexity of
${O(n).}$ The implementation is straightforward:

```rust
findMissing(array A):
	let high = A.maxElement();
	let H = new Array[high];
	for (i = 0; i < H.length; i++):
		H[i] = 0;
	for (i = 0; i < A.length; i++):
		H[A[i]]++;
	for (i = 0; i < H.length; i++):
		if (H[i] = 0):
			print "Missing element is (i)";
```

As an aside, the array ${H}$ is a very primitive example of a _hash table_.
We will revisit hash tables in a later section, but it's worth mentioning
this brief sighting as a precursor. Although the bitset is a clever
approach to search-and-rescue, it is not without its costs. The primary
concern for bitsets (and with hash tables as we'll see later), is the space
complexity. Given an array whose _maximum element_ is ${n,}$ the bitset
requires a length of ${n + 1.}$ This means that if the array ${A}$
contained the element ${1~000~000,}$ the bitset requires a length of
${1~000~001.}$ This is a large amount of memory.

## Duplicate Handling

Often, we want to eliminate, count, or store duplicate elements in a given
sequence. When we eliminate duplicates in a given sequence, are performing
a process called **uniquifying** or **deduplication**. Algorithms dedicated
to accomplishing this task are called **uniquifiers** (or _deduplicators_).
Uniquifiers are part of a broader area called **duplicate handling**.
Duplicate handling itself is particularly important when it comes to
databases. For example, ensuring that every user has a unique username or
identification number requires duplicate handling.

### Duplicate Spotters: Identifying Duplicates in a Sorted Sequence

The first step towards deduplication is to identify the duplicates.
Algorithms that identify duplicates are called **duplicate spotters**. Say
we had the following array:

<Sequence data={[3, 6, 8, 10, 12, 15, 15, 15, 20]} />

Immediately, we see the following duplicates: ${8, 8, 15, 15, 15.}$ To find
these duplicates, we scan through the list. Here, there are ${10}$
elements, but this could be any number of ${n}$ elements. Starting at
${i = 0,}$ we check if the element at that index, ${A_i}$ is the same as
the element at ${i = 1,}$ the element ${A_{i+1}.}$ Since ${A_i}$ and
${A_{i+1}}$ are not the same, we now check ${A_{i+1}}$ against ${A_{i+2}.}$
Again, they aren't the same, so we check ${A_{i+2}}$ against ${A_{i+3}.}$
Here we encounter our first duplicate, so we record the fact that the
element at ${i = 2}$ is duplicated.

We continue. Check ${A_{i+3}}$ against ${A_{i+4.}}$ No duplicate. Check
${A_{i+4}}$ against ${A_{i+5}.}$ No duplicates. Check ${A_{i+5}}$ against
${A_{i+6.}}$ Again no duplicates. Check ${A_{i+6}}$ against ${A_{i+7.}}$
Now we have a duplicate, so we record the fact that the element at ${i=6}$
is duplicated. Checking ${A_{i+7}}$ against ${A_{i+8,}}$ we have the same
duplicate. Now we've arrived at a peculiar situation. Do we record the
duplicate twice? No. We just want to know what the duplicate elements are.
It would be confusing and unnecessary to duplicate the duplicates.

One solution to this problem is to initialize a **walker**—a variable that
changes its value based on the current duplicate. The walker changes its
value only if the duplicate changes. And only when the walker changes do we
print its current value. The walker is initially zero. When we encounter
the first duplicate, ${8,}$ the walker changes and prints its value, ${8.}$
When we encounter the second duplicate, ${15,}$ the walker changes and
prints its value, ${15.}$ Since the second duplicate at ${i = 8}$ the same
duplicate, the walker doesn't change (and by contrapositive, doesn't
print). Implementing this approach, we call the walker `lastDuplicate`:

```rust
findDuplicates(array A):
	let last_duplicate = 0;
	for (let i = 0; i < A.length; i++):
		if ((A[i] = A[i+1]) && (A[i] != last_duplicate)):
			print "Duplicate: |A[i]|";
			last_duplicate = A[i];
```

With this implementation, we're iterating over each element in the array
${A,}$ performing a comparison. This means that given ${n}$ elements, this
algorithm takes ${n}$ operations to execute. As such, the running time
complexity is linear—${O(n).}$

## Useful Applications.

There are several useful applications for identifying duplicates:

- **Ensuring alternating values.** If we expect a given sequence to be
  ${\lang 1, 0, 1, 0, 1, \ldots \rang,}$ we can check the sequence follows
  the pattern by identifying any subsequence ${0,0}$ or ${1,1.}$ One way to
  do so is by using a duplicate spotter.

# Duplicate Counters: Counting the Number of Duplicates in a Sorted Sequence

Determining the number of duplicates is critical for duplicate handling.
Perhaps we only want to remove a certain number of duplicates. Perhaps our
program has a particular threshold for the number of duplicates. A
nightclub, for instance, might want to keep track of the ratio of female to
male. After a certain amount of duplicates of the string property `"male"`,
no further admissions are permitted for data with that property.

Like the previous algorithm, we can count the number of duplicates in a
sorted sequence through hashing. For example, say we had the following
array:

<Sequence data={[3, 6, 8, 8, 10, 12, 15, 15, 15, 20]} />

We see that the duplicated elements are ${8}$ and ${15.}$ The maximum
element here is ${20,}$ so we need bitset of length ${21:}$

<Sequence
	data={[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
/>

Then, all we have to do is traverse the array ${A.}$ For each element ${x}$
in ${A,}$ we increment the ${0}$ at the index in ${B}$ equal to ${x.}$ This
yields:

<Sequence
	data={[0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 1, 0, 1, 0, 0, 3, 0, 0, 0, 0, 1]}
/>

With this approach, all we're doing is scanning through the array ${A,}$
and incrementing each element in ${B}$ whose index equals the then-current
element in ${A.}$ If the array contains ${n}$ elements, this algorithm
takes ${n}$ comparisons to execute. As such, we have a time complexity of
${O(n).}$ This is linear time.

Implementing this algorithm:

```rust
findDuplicates(array A):
	let length_of_A = A.length;
	let A_max_element = A[length_of_A];
	let B = new Array[A_max_element];
	for (i = 0; i < A_max_element; i++):
		B[i] = 0;
	for (i = 0; i < length_of_A; i++):
		B[A[i]]++;
	for (i = 0; i < A_max_element; i++):
		if (B[i] > 1):
				Print "{i} has {B[i]-1} duplicates";
```

Note that while there are multiple for-loops in this implementation, each
loop simply takes ${n,}$ or some multiple of ${n,}$ operations to execute.
Accordingly, the implementation above takes some ${k \cdot n}$ operations
to execute, where ${k}$ is some constant. Thus, our original analysis is
unchanged: the algorithm has a running time complexity of ${O(n).}$

## Finding the Total Number of Duplicates.

With this implementation, counting the number of duplicate elements is
trivial. If we wanted to find the total number of duplicate elements, we
simply traverse the array ${B,}$ and summate the elements greater than
${1:}$

```rust
findDuplicates(array A):
	let total_duplicates = 0;
	let length_of_A = A.length;
	let A_max_element = A[length_of_A];
	let B = new Array[A_max_element];
	for (i = 0; i < A_max_element; i++):
		B[i] = 0;
	for (i = 0; i < length_of_A; i++):
		B[A[i]]++;
	for (i = 0; i < A_max_element; i++):
		if (B[i] > 1):
			total_duplicates += B[i];
	return total_duplicates;
```

Again, this doesn't change our complexity analysis. The algorithm above
still takes ${O(n)}$ time.

## Duplicate Counter: Counting Duplicates in an Unsorted Sequence

The previous algorithm counted the number of duplicates in a sorted
sequence. How might we count the number of duplicates in an _unsorted_
sequence? Consider this unsorted array:

<Sequence data={[8, 3, 6, 4, 6, 5, 6, 8, 2, 7]} />

Above, we see the duplicates are ${8}$ and ${6.}$ To count these
duplicates, we use the following procedure:

1. Instantiate a variable called `count`. This variable will increment each
   time a duplicate is encountered.
2. Instantiate two integer variables, ${i = 0}$ and ${j = i + 1.}$
3. As long as ${i < j}$, perform the following:
   1. Compare ${A_i}$ against ${A_j:}$
   2. If ${A_i \neq A_j,}$ increment ${j.}$
   3. Otherwise (i.e., ${A_i}$ = ${A_j}$):
      1. Increment `count`.
      2. Replace ${A_j}$ with ${\texttt{-}1.}$
      3. Increment ${j.}$

Notice that in step 3.1.2.2, we replace ${A_j}$ with ${\texttt{-}1.}$ We
use the value ${\texttt{-}1}$ to mark the duplicate element as accounted
for. If we don't include this marking step, we would be duplicating our
count. For example, when we reach the duplicate ${6,}$ we would increment
`count` after encountering the second ${6,}$ and increment again after
encountering the third ${6.}$ When we reach the second ${6,}$ we would
increment `count` again when we encounter the third ${6,}$ even though
we've already counted that ${6.}$ By replacing the values with
${\texttt{-}1,}$ we ensure that we don't count duplicates that have already
been counted. Implementing this algorithm:

```rust
countDuplicates(array A):
		let B = new Array[A.length];
		let count = 0;
		for (i = 0; i < B.size; i++): B[i] = A[i];
		for (i = 0; i < A.length - 1; i++):
			for (j = i + 1; i < A.length; j++):
				if (A[i] == A[j]):
				count++;
				B[j] = -1
			if (count > 0):
				print "Element {A[i]} is duplicated {count} times"
```

The time complexity for this algorithm is ${O(n^2).}$ Because we start by
comparing the first element against all the others in an array of ${n}$
elements, the algorithm performs ${n - 1}$ comparisons at first. After
comparing the first element against the others, the algorithm moves to the
second. This yields ${n - 2}$ comparisons. After the second, the algorithm
moves to the third, performing ${n - 3}$ comparisons. The pattern:

$$
	\large \lang n-1,~n-2,~n-3,~n-4,~n-5,~\ldots,~3,~2,~1 \rang
$$

The sum of this sequence:

$$
	\begin{aligned} \sum\limits_{k = 1}^{n - 1} &= (n - 1) + (n - 2) +
	\ldots + 3 + 2 + 1 \\[1em] &= 1 + 2 + 3 + \ldots + (n - 2) + (n - 1)
	\\[1em] &= \dfrac{n(n-1)}{2} \\[1em] &= \dfrac{n^2 - n}{2}
	\end{aligned}
$$

Thus, the running time function of this algorithm is
${f(n) =
\dfrac{n^2 - n}{2}.}$ Applying asymptotic analysis, we see that
this algorithm runs on quadratic time—${O(n^2).}$

Alternatively, we can use a bitset, just as we did with the sorted
sequences. Using the same unsorted array:

<Sequence data={[8, 3, 6, 4, 6, 5, 6, 8, 2, 7]} />

We create a bit set with a size equal to the largest element of ${A}$ plus
one. In this case, that element is ${8,}$ so we need bitset of size ${9:}$

<Sequence data={[0, 0, 0, 0, 0, 0, 0, 0, 0]} />

The procedure is the same as we saw previously for sorted sequences. We
iterate through the array ${A,}$ and for each element ${x}$ in ${A,}$ we
increment the ${0}$ at the index ${j = x.}$ This results in:

<Sequence data={[0, 0, 1, 1, 1, 1, 3, 1, 2]} />

The implementation for this approach is the same as we saw earlier:

```rust
findDuplicates(array A):
	let length_of_A = A.length;
	let A_max_element = A[length_of_A];
	let B = new Array[A_max_element];
	for (i = 0; i < A_max_element; i++):
		B[i] = 0;
	for (i = 0; i < length_of_A; i++):
		B[A[i]]++;
	for (i = 0; i < A_max_element; i++):
		if (B[i] > 1):
			Print "{i} has {B[i]-1} duplicates";
```

Compared to the previous approach, all that's required for this algorithm
is to iterate through ${n}$ elements and incrementing the element at the
corresponding index in the bitset. Hence, this algorithm has a time
complexity of ${O(n).}$ This is linear time, a faster approach compared
with the quadratic time for the previous algorithm.

## Pair to Sum: Unsorted Arrays

Consider the following problem:

> Given an array of ${n}$ integers and an integer ${k,}$ find all pairs
> ${(a, b)}$ in the array such that ${a + b = k.}$

The brute-force approach to solving this algorithm is to iterate through
each of the elements. For example, consider the following array:

<Sequence data={[6, 3, 8, 10, 16, 7, 5, 2, 9, 14]} />

Suppose ${k = 10.}$ Just looking at the array, we see that there is, in
fact, a pair ${(a, b)}$ such that ${a + b = 10.}$ In this case, ${a =
3}$
and ${b = 7.}$ With the brute force approach, we employ the following
algorithm:

1. Suppose ${k}$ is the sum.
2. For ${i = 0, 1, 2, 3, \ldots, n-1}$ perform the following:
   1. Let ${d = k - A_i.}$
   2. Let ${j = i + 1.}$
   3. For ${j = 1, 2, 3, \ldots, n,}$ perform the following:
      1. If ${A_j = d,}$ print ${(A_i, A_j).}$
      2. Otherwise, increment ${j}$ and return to step 2.3.

Implementing this algorithm:

```rust
sumPairFinder(array A, sum):
	for (i = 0; i < A.length - 1; i++):
	let d = sum - A[i];
		for (j = i + 1; j < A.length; i++):
			if (A[j] = d): print "Pair: A[i] + A[j] = k";
```

Like one of our previous duplicate counters, this algorithm relies on a
nested for-loop to find a pair. Hence, given an array of ${n}$ elements,
this algorithm has a running time complexity of ${O(n^2).}$ Note, however,
that this algorithm only works if the array contains no duplicates. If the
array contains duplicates, then we must use one of the duplicate spotters
and remove the duplicate elements before employing this approach.

Alternatively, we can use a bitset to find the pair ${(a, b).}$ Using the
same unsorted array:

<Sequence data={[6, 3, 8, 10, 16, 7, 5, 2, 9, 14]} />

We see that the largest element is ${16,}$ so we need a bit set of size
${17:}$

<Sequence data={[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]} />

Now, rather than simply iterating over ${A}$ and incrementing the
corresponding ${0}$ in ${B,}$ the algorithm performs the following:

1. Iterate through ${A:}$
   1. Compute the difference ${d = k - A_i.}$
   2. Check if the element ${B_d \neq 0.}$ If it is, return ${B_d}$ and
      ${A_i.}$
   3. Increment element ${B_j}$ where ${j = A_i.}$

The final step, checking if ${B_d > 0,}$ is how we determine if the other
term, ${b,}$ is present in the array. By the time we reach the element
${7}$ and increment ${B_7,}$ the bitset looks like:

<Sequence data={[0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]} />

And at this point, ${d = 3.}$ Checking if ${B_3}$ is greater than zero, we
see that it's true, and we have our first pair, ${(7, 3).}$ When we get to
${A_7,}$ we see that ${B_8 > 0,}$ and we find the second pair, ${(2, 8).}$
Implementing this algorithm:

```rust
pairSumFinder(array A, k):
	let B = new array[A.length];
	for (i = 0; i < B.length; i++):
		B[i] = 0;
	for (i = 0; i < A.length; i++):
		let d = k - A[i];
		if (B[d] != 0): print "Pair: A[i] + A[j] = k"
		B[A[i]]++;
```

As we know from hashing, this algorithm requires iterating through each of
the elements in ${n,}$ and the operations at each iteration run in constant
time. Accordingly, this approach has a time complexity of ${O(n)}$—linear
time, which is better than our previous approach running on quadratic time.

## Pair to Sum: Sorted Arrays

The algorithms above found pairs in unsorted sequences. What about sorted
sequences? For example, suppose we were asked to find the pairs in this
sorted sequence:

<Sequence data={[1, 3, 4, 5, 6, 8, 9, 10, 12, 14]} />

Looking at this array, we see two pairs: ${(1, 9)}$ and ${(4, 6).}$ Because
all of the elements are sorted in ascending order, we have the smaller
elements to the left and the larger elements to the right. From real
analysis, we know that given a pair of real numbers ${(a, b),}$ there are
only two possibilities—(1) either ${a = b,}$ or (2) one of the elements is
smaller than the other. Because there are only two possibilities, and the
fact that the array is already sorted, we can search for an ${a}$ and a
${b}$ such that ${a + b = k,}$ from _both sides_ of the array.

Applying these facts, the algorithm proceeds as such:

1. Create a variable ${i.}$ Starting at the first index of ${A,}$ ${i}$
   will be incremented (if necessary) until it reaches
   ${i =
\text{ A's length}.}$

2. Create a variable ${j.}$ Starting at the last index of ${A,}$ ${j}$ will
   be decremented (if necessary) until it reaches ${j = 0.}$

3. As long as ${i \neq j:}$
   1. If ${A_i + A_j = k:}$
      1. Return ${A_i}$ and ${A_j.}$ These are pairs.
      1. Increment ${i}$ and decrement ${j.}$
   2. Otherwise:
      1. If ${A_i + A_j < k,}$ increment ${i.}$
      2. Else, decrement ${j.}$

Implementing the algorithm in pseudocode:

```rust
findPairInSorted(array A, k):
	int i = 0;
	int j = A.length;
	while (i &NotEqual; j):
		if (A[i] + A[j] = k):
			print "Pairs: A[i] + A[j] = k";
			i++, j--;
		else if (A[i] + A[j] < k): i++;
		else: j--;
```

Examining the code above, this algorithm only requires iterating through
the array ${A.}$ The operations inside the while-loop's body run in
constant time, so this algorithm has a time complexity of ${O(n).}$

## Finding a Minimum and a Maximum in a Single Scan

With the algorithm's we've seen so far, we likely noticed how useful it is
to know what the maximum and minimum elements are for an unsorted sequence.
Rather than calling two separate functions that find the minimum and
maximum respectively, we can write a single function that returns both. For
example, say we had the following unsorted array:

<Sequence data={[5, 8, 4, 9, 6, 2, 10, 7, -1, 4]} />

We see that the maximum in this array is ${10,}$ and the minimum is ${-1.}$
The trick to finding these elements is to first instantiate two variables,
`min` and `max`. Initially, the both of these variables are assigned the
first element in the array. In this case, ${5:}$

```rust
min = A[0]
max = A[0]
```

Then, we begin iterating, from `i = 0` to
`i = ${\text{length of \texttt{A}},}$` performing the following:

1. Is `A[i]` less than `min`? If it is, change the value of `min` to `A[i]`
   and go to step 3. Otherwise, ask the next question.

2. Is `A[i]` greater than `max`? If it is, change the value of `max`.
   Otherwise, go to step 3.

3. If `i` is less than the length of `A` increment `i` and return to
   step 1.

4. Otherwise, return `min` and `max`.

Implementing this algorithm:

```rust
findMinMax(array A):
	let min = A[0];
	let max = A[0];
	for (i = 0; i < A.length; i++):
		if (A[i] < min): min = A[i];
		else if (A[i] > max): max = A[i];
	return (min, max);
```

With this algorithm, all we are doing is iterating over ${n}$ elements. The
comparison operators run in constant time, so they do not impact our
analysis. Hence, this algorithm has runs on linear time: ${O(n).}$

As an aside, it's worth considering how many comparisons are made. In the
best-case scenario, the array `A` is sorted in ascending order. In that
scenario, the `if` block alone is executed. Given that we're comparing
against every element other than the first, we have a total of ${n - 1}$
comparisons. In the worst-case scenario, the array `A` is sorted in
descending order. There, the the `if` block is executed _and_ the `else if`
block is executed. This yields ${2(n-1)}$ comparisons. Applying asymptotic
analysis, the best-case and the worst-case scenarios are the same—we end up
with linear time, either or.

The more important point, however, is that there's distinction between an
algorithm's cases and the algorithm's time complexities. The _worst case_
for this algorithm is if the array is in descending order, and the _best
case_ for this algorithm is if the array is in ascending order. Being aware
of this difference goes a long way. There are situations where the time
complexity is less important than the algorithm's cases. The algorithm
might have the same time complexity across all cases, but the worst case
specifically occurs far more often. Knowing this fact can impact other
aspects of the algorithm's assessment, whether that's space complexity,
readability, heuristics, etc.

## Array Addressing

Whenever we write expressions that index into an array, e.g., `A[2] = 4`,
the compiler must translate the expression into an address. Because arrays
are a fundamental data structure, it's worth knowing how this translation
is done. Put simply, the compiler makes performs this translation by
applying a particular formula. For example, suppose we initialized the
following array:

```c
int main() {
	int A[3] = {8, 3, 5};
	return 0;
}
```

As we know, the identifier `A` is an identifier for array's base
address—the address of `A[0]`. Suppose the address of `A[0]` is `200`. This
means that the address of `A[1]` is `204` (since an `int` takes 4 bytes),
an the address of `A[2]` is `208`. Abstracting this computation, we can
think of the compiler performing the following when it encounters `A[2]`:

$$
	\begin{aligned} \textit{address-of}(A[2])&=
	\textit{address-of}(A[0]) + (2)(4) \\ &= 200 + 8 \\ &= 209
	\end{aligned}
$$

We can abstract this computation into a formula:

$$
	\alpha(a_i) = \alpha(a_0) + i \omega
$$

In the formula above, ${\alpha(n)}$ is a function that returns the memory
address of some data ${n}$. Thus, ${\alpha(a_i)}$ returns the address of
the ${i^{\text{\scriptsize{th}}}}$ element of the array, and
${\alpha(a_0)}$ returns the address of the first element. The variable
${i}$ represents the index of the element, and the variable ${\omega}$
represents the size of the data type (e.g., the size of the data type `int`
is 4 bytes).

As an aside, many older languages like Fortran use 1-based indexing. C came
after Fortran, so why do so many languages use 0-based indexing? One reason
is because of the hardware limitations at the time languages like C, BCPL,
and Fortran were implemented. With 1-based indexing, we would have to use a
different formula in determining the address of a given element:

$$
	\alpha(a_i) = \alpha(a_1) + (i-1)\omega
$$

There's an additional computation with this formula—a decrement. This
additional operation proves to be costly on older machines—there are 3
separate computations. Compare that with the previous formula, which calls
for only 2 computations. Given that a core goal of C's design was
efficiency, it made sense to opt for the more efficient implementation.

Of course, modern computers have surpassed many of these limitations. The
additional decrementing operation doesn't make much of a difference for
most applications. So why then do recent languages go with zero-based
indexing? Because zero-based indexing has become the norm. C inspired a
whole host of languages (e.g., C++ and Objective-C), which in turn inspired
many others (e.g., C to C++ to Java to Kotlin, C to Objective-C to Swift).
Given that language designers are programmers first and foremost, it isn't
all that surprising to see a designer sticking with what they're familiar
with.

Moreover, if often makes more sense to use zero-based indexing over
one-based indexing. We can appreciate this idea by recognizing that
_indexing_ is _not_ the same as _counting_. Instead, indexing is much more
akin to _offsetting_ from a given point. For example, given the array
`int A[3] = {1, 2, 3}`, the starting point is `1`. If we designate that as
the element with index 0 (`A[0]`, the first element), then the second
element, `A[1]`, is the element 1-off the first element. The third element,
`A[2]`, is 2-off the first element. And so on and so forth.
