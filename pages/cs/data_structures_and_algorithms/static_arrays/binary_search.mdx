import { Sequence } from "@illus/Sequence";
import { HTree } from "@illus/HTree";

<Metadata
	title={"Binary Search"}
	description={"Notes on binary search."}
	keywords={"Binary search, static arrays, logarithmic time."}
/>

# Binary Search

The **binary search** algorithm is correct only if the array is _sorted_.
There are a whole host of sorting algorithms, so we will cover them in a
separate section. For now, suppose we have the following sorted array:

<Sequence
	data={[4, 8, 10, 15, 18, 21, 24, 27, 29, 33, 34, 37, 39, 41, 43]}
/>

Suppose our key is ${k = 18.}$ We see that 18 is at index ${i = 4.}$ With
linear search, we must perform 5 comparisions before ${k}$ is found. Let's
see how binary search compares.

First, with binary search, we need three variables: ${\ell,}$ ${h,}$ and
${M.}$ These variables represent three variants: ${\ell}$ represents the
lower bound (an index), ${h}$ represents the upper bound (also an index),
and ${M}$ represents the index exactly between ${\ell}$ and ${h.}$
Accordingly, the value of ${M}$ is given by:

$$
	M = \lfloor \dfrac{\ell + h}{2} \rfloor
$$

This simply the arithmetic mean, but we will floor the result to account
for the possibility of an odd number of elements in the array. For example,
if the array has 5 elements, then ${\ell = 0,}$ and ${h = 5.}$ Calculating
${M:}$ ${(0 + 5) / 2 = 2.5.}$ Obviously an index cannot be ${2.5,}$ so we
floor the result: ${M \equiv i = 2.}$

That said, let's trace our program. We know ${k = 18.}$ So, we first
initialize ${\ell}$ and ${h:}$

$$
\begin{aligned}
	\ell &= 0 \\
	h &= 14
\end{aligned}
$$

Then we compute ${M:}$

$$
	M = \dfrac{0 + 14}{2} = 7
$$

Thus, ${M}$ is ${i = 7.}$ We then ask, is ${T[7] = k?}$ If yes, the search
is successful, and we return the index 7. Otherwise, we ask, is
${T[7] < k}$ or is ${T[7] > k?}$ Here, ${T[7] < k.}$ Because ${T[7] < k,}$
${k}$ must be on the left-hand side of the array. I.e.,
${\ell \leq k < M.}$ Why can we conclude this? Because the array ${T}$ is
sorted. If ${T[7] > k,}$ then ${k}$ must be on the right-hand side
(${M < k \leq h}$).

In this case, since ${T[7] < k,}$ ${\ell}$ remains as ${i = 0,}$ but we
change ${h}$ to ${i = 6.}$ Now we're working the left-hand side, which is
${T[0] < k \leq T[6].}$ Now we have a new mean: ${(0 + 6) / 2 = 3.}$ Thus,
now the middle element is ${T[3] = 15.}$ We again ask, is ${k = T[3]?}$ No.
Is ${k < T[3]}$ or is ${k > T[3]?}$ Here, ${18 > 15,}$ so we're looking at
the right-hand side of the array.

Because ${T[M] > k,}$ we again change the lower bounds and upper bounds.
${\ell = 4,}$ and ${h = 4.}$ The mean is now ${M = (4 + 4) / 2 = 4.}$ Thus,
the middle element is now ${T[4] = 18.}$ We now ask, is ${k = T[4]?}$ Yes.
We have found our key. Notice that we found our key in a total of 4
comparisons, compared to 5 comparisons with linear search.

Let's try another key. ${k = 34.}$ We set the upper bounds and lower
bounds. ${\ell = 0}$ and ${h = 14.}$ Then we compute the mean index.
${(0 + 14) / 2 = 7.}$ Again we have ${T[7] = 27.}$ Is ${T[M] = k?}$ No. Is
${T[M] > k}$ or is ${T[M] < k?}$ ${27 < 34.}$ Thus, we're now restricting
our search to the right-hand side.

We change the upper and lower bounds. ${\ell = M + 1 = 7 + 1 = 8.}$ And
${h = 14.}$ Then compute the mean index: ${(8 + 14) / 2 = 11.}$ Thus, the
middle element is ${T[M] = T[11] = 37.}$ Is ${T[M] = k?}$ No. Is
${T[M] > k}$ or ${T[M] < k?}$ ${37 > 34,}$ so we're looking at the
left-hand side.

We again change the upper and lower bounds. ${\ell = 8}$ and
${h = 11 - 1 = 10.}$ Computing the mean: ${(10 + 8) / 2 = 9.}$ Is
${(T[9] = 33. = 34?}$ No. Is ${33 > 34}$ or is ${33 < 34?}$ ${34}$ is less
than ${33.}$ So, the number is on the right-hand side. Once more we modify
the upper and lower bounds.

${\ell = M + 1 = 9 + 1 = 10,}$ and ${h = 10.}$ We compute the mean:
${(10 + 10) / 2 = 10.}$ Is ${T[10] = 34?}$ Yes. We've found our key in 4
comparisons. Compare that with linear search, which would have taken 10
comparisions.

Let's try another key, ${k = 25.}$ Set the upper and lower bounds:
${\ell = 0}$ and ${h = 14.}$ Compute the mean: ${M = (0 + 14) = 7.}$
Compare ${T[7]}$ to ${k.}$ ${k}$ is not equal to ${T[7].}$ ${k < T[M],}$ so
we're looking at the left-hand side.

Change the upper and lower bounds. ${\ell = 0,}$ and ${h = 7 - 1 = 6.}$
Compute the mean: ${M = (0 + 6)/2 = 3.}$ ${T[3]}$ is not equal to ${k.}$
Because ${k > T[M],}$ we're looking at the right-hand side. So we change
the upper and lower bounds.

${\ell = 3 + 1 = 4,}$ and ${h = 6.}$ We compute the mean:
${(4 + 6) / 2 = 5.}$ The middle element is ${T[5].}$ ${T[5]}$ is not equal
to ${25,}$ ${k > T[5].}$ So again we're looking at the left-hand side. Once
more we modify the lower and upper bounds.

${\ell = 5 + 1 = 6,}$ and ${h = 6.}$ The mean: ${(6 + 6) / 2 = 6.}$
${T[6]}$ is not equal to ${25.}$ ${25 > T[6],}$ so we modify the upper and
lower bounds.

${\ell = 6 + 1 = 7,}$ and ${h = 6.}$ This is where we stop. The moment
${\ell > h,}$ we've arrived at an unsuccessful search, so we return ${-1.}$
Notice that it took 4 comparisons to reach an unsuccessful search, compared
to the 15 comparisions it would've taken with linear search.

Implementing binay search in pseudocode:

```rust
BinarySearch(k, low, high) {
	while (low <= high) {
		middle = floor((low + high) / 2);
		if (k == T[mean]) {
			return mean;
		}
		else if (k < T[mean]) {
			high = mean - 1;
		}
		else {
			low = mean + 1;
		}
	}
	return -1;
}
```

Alternatively, we can implement the binary search algorithm with tail
recursion:

```rust
BinarySearch(k, low, high) {
	if (low <= high) {
		mean = floor((low + high) / 2);
		if (k == T[mean]) {
			return mean;
		}
		else if (key < T[mean]) {
			return BinarySearch(low, mean - 1, key);
		}
		else {
			return BinarySearch(mean + 1, high, key);
		}
	}
	return -1;
}
```

To understand the time complexity for binary search, the crucial point is
that we're cutting the array in half each time. However, before any of that
splitting, we checked if the middle element is in fact the key.
Accordingly, in the best-case scenario, binary search has a time complexity
of ${\Omega(1)}$—constant time.

However, if the middle element was not the key, then we proceed to checking
if the left- or right-hand side contained the key. Suppose it's the
left-hand side. If it's the left-hand side, then ${\ell = 0}$ and
${h = 6,}$ yielding ${M = 3.}$ If it's on the right-hand side, then
${\ell = 8}$ and ${h = 14,}$ yielding ${M = 11.}$ This is halving the array
and restricting our search to just the relevant half. If the middle element
in either half is the key, then we've found our key in 2 comparisons.

If neither of the halves contains the middle element, then we again perform
the split. For the left-hand side of the left-hand side, ${\ell = 0}$ and
${h = 2,}$ yielding ${M = 1.}$ If it's the right-hand side of the left-hand
side, then ${\ell = 4}$ and ${h = 6,}$ yielding ${M = 5.}$ For the
left-hand side of the right-hand side, we have ${\ell = 8}$ and ${h = 10,}$
yielding ${M = 9.}$ For the right-hand side of the right-hand side, we have
${\ell = 12}$ and ${h = 14,}$ yielding ${M = 13.}$ In this case, the key
can be found in 3 comparisons. Visualizing the tree:

<HTree
	data={[
		["T[7]=27"],
		["T[7]=27", "T[3]=15", "T[11]=37"],
		["T[3]=15", "T[1]=8", "T[5]=21"],
		["T[11]=37", "T[9]=33", "T[13]=41"],
		["T[1]=8", "T[0]=4", "T[2]=10"],
		["T[5]=21", "T[4]=18", "T[6]=24"],
		["T[9]=33", "T[8]=29", "T[10]=34"],
		["T[13]=41", "T[12]=39", "T[14]=43"],
	]}
	height={700}
	width={650}
/>

Splitting further, the key can be found in 4 comparisions, then 5, then 6,
and so on. Modelling binary search as a tree, the height of the tree is
${\lceil\log_{2}(n + 1)\rceil,}$ where ${n}$ is the number of elements in
the array. Why ${\log_{2}(n + 1)?}$ Because the tree's height is directly
proportional to the number of times we divide ${n}$ by 2 such that we only
have a remainder of 1. And as we know from mathematics, the answer to "How
many times can I divide ${n}$ by 2 to reach a remainder of 1?" is a
logarithm of base 2. Successive multiplication yields a power, and
successive division yields a logarithm. Thus, in terms of asymptotic
analysis, the binary search algorithm has a runtime of order ${O(\lg n)}$
—logarithmic time.

What about the average-case? For the average-case, we're asking for the
total time for all the possible cases, divided by the number of cases.
Revisiting the tree diagram above, we see that at the first level, there is
only 1 case, with only 1 comparison. Let ${c}$ be the number of cases:

$$
	c = 1
$$

On the second level, there are 2 cases, each requiring 1 comparison:

$$
	c = 1 + (1 \cdot 2)
$$

On the third level, there are 4 cases, each requiring 2 comparisons:

$$
	c = 1 + (1 \cdot 2) + (2 \cdot 4)
$$

On the fourth level, there are 8 cases, each requiring 3 comparisons:

$$
	c = 1 + (1 \cdot 2) + (2 \cdot 4) + (3 \cdot 8)
$$

We can see pattern in this series:

$$
	c = 1 + (1 \cdot 2^1) + (2 \cdot 2^2) + (3 \cdot 2^3)
$$

We can then express this pattern as:

$$
	\sum\limits_{i = 1}^{3} i \cdot 2^i
$$

That 3 is intresting—it's the tree's height. Accordingly, our series can be
generalized as:

$$
	\sum\limits_{i = 1}^{\log n} i \cdot 2^i = \log n \cdot 2^{\log n}
$$

But, we must divide this sum by ${n,}$ the number of elements (and by
implication, the number of possible cases):

$$
	\dfrac{\log n \cdot 2^{\log n}}{n} = \log n
$$

Hence, the average-case running time is ${\Theta(\log n)}$ An algorithm's
average-case, however, may be different depending on the conditions. For
example, with respect to binary search, there's an average-case running
time for a successful search, and an average-case running time for an
unsuccessful search.

A successful search occurs if the element is found. In the tree diagram
above, the number of possible cases for a successful search is equivalent
to the number of internal nodes (i.e., all of the nodes other than the
nodes on the fifth level). If we examine the tree closely, the number of
edges from the root to a given node represents the number of comparisons
needed to reach the given node. For example, to find the ${k = 8,}$
binary-search requires 3 comparisons. Accordingly, the number of
comparisons to reach a given node containing ${k}$ is ${e + 1,}$ where
${e}$ is the number of edges leading to the node from the root (we add
${1}$ because a comparison must be performed for the root).

The number of possible cases for an unsuccesful search, in contrast, is
found from the number of nodes on the fifth level. These are called
**external nodes**. It is on these external nodes that binary search
terminates. Like the successful search, the number of comparisons needed to
reach an unsuccessful search is the number of edges leading to an external
node plus 1.

With these basic facts, suppose that ${I}$ represents the sum of all the
paths leading to internal nodes and ${E}$ represents the sum of the paths
leading to external nodes. It follows then that:

$$
	E = I + 2n
$$

where ${n}$ is the number of internal nodes. Why ${+ 2n?}$ Because each of
the internal nodes have 2 children (since the binary search algorithm works
by successively dividing the problem by 2). For example, here's a simple
binary tree:

![Internal versus external nodes on a binary tree.](https://res.cloudinary.com/sublimis/image/upload/v1652749054/cs/binary_tree_internal_versus_external_nodes.svg)

Notice that ${n = 3.}$ Thus, ${2n = 6.}$ Then, notice that ${I = 2.}$
Hence, ${E = 2 + 6 = 8.}$ Indeed, there are 8 paths from the root to the
external nodes.

Alongside the facts above, we also know that the number of external nodes
is the number of internal does plus 1: ${e = n + 1,}$ where ${e}$ is the
number of external nodes, and ${n}$ is the number of internal nodes.
Knowing all of this, the average-case runtime for a successful search on an
${n}$ element array is the sum of the paths to internal nodes divided by
the number of nodes (the sum of all possible cases divided by the number of
cases) plus 1 (including the comparison for the root node):

> _Lemma_. ${\Omega\left(1 + \dfrac{I}{n}\right)}$, where ${I}$ is the
> sum > of all paths to internal nodes and ${n}$ is the number of nodes.

Similarly, the average-case runtime for an unsuccesful search is:

> _Lemma_. ${\Omega\left( \dfrac{E}{n + 1} \right),}$ where ${E}$ is the >
> sum of all paths to external nodes and ${n}$ is the number of nodes

Note that the sum of all paths to external nodes is roughly the height of
the tree—${n \log n.}$ Accordingly we can generalize the average-case
runtime for an unsuccessful search as:

$$
	\Omega\left( \dfrac{n \log n}{n + 1} \right)
$$

Generalizing even further:

$$
	\Omega(\log n)
$$

Using what we know about ${E,}$ we can use substitution to find a more
generalized version of the average-case runtime for a successful search.
Where ${A_s(n)}$ is the average-case runtime for a successful search:

$$
	\begin{aligned} A_s(n) &= 1 + \dfrac{I}{n} \\[1em] &= 1 + \dfrac{E - 2n}{n} \\[1em] &= 1 + \dfrac{E}{n} - 2 \\[1em] &= 1 + \dfrac{n \log n}{n} - 2 \\[1em] &= 1 + \log n + 2 \end{aligned}
$$

Applying asymptotic analysis, the average-case runtime for a successful
search is:

$$
	\Omega (\log n)
$$

Here's an implementation in Java:

```java
public static int binarySearch(int[] a, int key) {
	int lo = 0;
	int hi = a.length - 1;
	while (lo <= hi) {
		int mid = lo + ((hi-lo) / 2);
		if (key < a[mid]) hi = mid - 1;
		else if (key > a[mid]) lo = mid + 1;
		else return mid;
	}
}
```
